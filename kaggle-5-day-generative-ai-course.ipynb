{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":30786,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2024-11-12T18:51:16.415997Z","iopub.execute_input":"2024-11-12T18:51:16.417034Z","iopub.status.idle":"2024-11-12T18:51:16.423870Z","shell.execute_reply.started":"2024-11-12T18:51:16.416984Z","shell.execute_reply":"2024-11-12T18:51:16.422660Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"%pip install -U -q \"google-generativeai>=0.8.3\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-12T18:51:16.425611Z","iopub.execute_input":"2024-11-12T18:51:16.425929Z","iopub.status.idle":"2024-11-12T18:51:27.646809Z","shell.execute_reply.started":"2024-11-12T18:51:16.425896Z","shell.execute_reply":"2024-11-12T18:51:27.645421Z"}},"outputs":[{"name":"stdout","text":"Note: you may need to restart the kernel to use updated packages.\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"import google.generativeai as genai\nfrom IPython.display import HTML, Markdown, display","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-12T18:53:30.751870Z","iopub.execute_input":"2024-11-12T18:53:30.752333Z","iopub.status.idle":"2024-11-12T18:53:31.840330Z","shell.execute_reply.started":"2024-11-12T18:53:30.752281Z","shell.execute_reply":"2024-11-12T18:53:31.839444Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"from kaggle_secrets import UserSecretsClient\n\nGOOGLE_API_KEY = UserSecretsClient().get_secret(\"GOOGLE_API_KEY\")\ngenai.configure(api_key=GOOGLE_API_KEY)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-12T18:54:03.592444Z","iopub.execute_input":"2024-11-12T18:54:03.593273Z","iopub.status.idle":"2024-11-12T18:54:03.677890Z","shell.execute_reply.started":"2024-11-12T18:54:03.593225Z","shell.execute_reply":"2024-11-12T18:54:03.676819Z"}},"outputs":[],"execution_count":7},{"cell_type":"code","source":"flash = genai.GenerativeModel('gemini-1.5-flash')\nresponse = flash.generate_content(\"Explain AI to me like I'm a kid.\")\nprint(response.text)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-12T18:55:54.611995Z","iopub.execute_input":"2024-11-12T18:55:54.612789Z","iopub.status.idle":"2024-11-12T18:55:56.155817Z","shell.execute_reply.started":"2024-11-12T18:55:54.612745Z","shell.execute_reply":"2024-11-12T18:55:56.154773Z"}},"outputs":[{"name":"stdout","text":"Imagine you have a really smart robot friend who can learn things just like you! That's what AI, or Artificial Intelligence, is all about.\n\nThink of AI like a super brain that can do lots of cool things, like:\n\n* **Playing games:** AI can play games like chess or video games really well because it can learn from its mistakes and get better over time.\n* **Understanding your words:** AI can listen to you talk or read your words and understand what you mean, like when you ask Siri or Alexa a question.\n* **Helping doctors:** AI can look at pictures and help doctors find problems in your body, like if you need to see a doctor or if you're healthy.\n* **Making your life easier:** AI helps with lots of everyday things, like finding the best route in your car or suggesting movies you might like to watch.\n\nIt's like having a helper who can learn and get smarter as it works! But remember, AI is just a tool, and it needs people to help it learn and make sure it's used in a good way. \n\n","output_type":"stream"}],"execution_count":8},{"cell_type":"code","source":"chat = flash.start_chat(history=[])\nresponse = chat.send_message('Hello! My name is Zlork.')\nprint(response.text)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-12T18:56:39.849530Z","iopub.execute_input":"2024-11-12T18:56:39.850245Z","iopub.status.idle":"2024-11-12T18:56:40.232188Z","shell.execute_reply.started":"2024-11-12T18:56:39.850204Z","shell.execute_reply":"2024-11-12T18:56:40.231144Z"}},"outputs":[{"name":"stdout","text":"Hello Zlork! It's nice to meet you. ðŸ˜Š  What can I do for you today? \n\n","output_type":"stream"}],"execution_count":9},{"cell_type":"code","source":"response = chat.send_message('Can you tell something interesting about dinosaurs?')\nprint(response.text)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-12T18:56:53.972251Z","iopub.execute_input":"2024-11-12T18:56:53.973412Z","iopub.status.idle":"2024-11-12T18:56:54.925030Z","shell.execute_reply.started":"2024-11-12T18:56:53.973342Z","shell.execute_reply":"2024-11-12T18:56:54.923964Z"}},"outputs":[{"name":"stdout","text":"Okay, Zlork! Here's something interesting about dinosaurs: \n\nDid you know that some dinosaurs were actually covered in feathers, not scales?  \n\nIt's true!  Scientists have found fossils of dinosaurs like Velociraptor and  Triceratops with clear evidence of feathers.  These feathers weren't always for flying, they might have been used for insulation, display, or even attracting mates.  \n\nIt's amazing to think that these giant creatures we often picture as scaly monsters might have actually been more colorful and feathery than we imagined! ðŸ¦– \n\n","output_type":"stream"}],"execution_count":10},{"cell_type":"code","source":"# While you have the `chat` object around, the conversation state\n# persists. Confirm that by asking if it knows my name.\nresponse = chat.send_message('Do you remember what my name is?')\nprint(response.text)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-12T18:57:16.766795Z","iopub.execute_input":"2024-11-12T18:57:16.767172Z","iopub.status.idle":"2024-11-12T18:57:17.189547Z","shell.execute_reply.started":"2024-11-12T18:57:16.767139Z","shell.execute_reply":"2024-11-12T18:57:17.188453Z"}},"outputs":[{"name":"stdout","text":"Of course! You are Zlork.  I remember you and our conversation about dinosaurs. ðŸ˜„  Is there anything else you'd like to talk about? \n\n","output_type":"stream"}],"execution_count":11},{"cell_type":"code","source":"for model in genai.list_models():\n  print(model.name)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-12T18:57:42.117417Z","iopub.execute_input":"2024-11-12T18:57:42.118444Z","iopub.status.idle":"2024-11-12T18:57:42.236210Z","shell.execute_reply.started":"2024-11-12T18:57:42.118374Z","shell.execute_reply":"2024-11-12T18:57:42.235209Z"}},"outputs":[{"name":"stdout","text":"models/chat-bison-001\nmodels/text-bison-001\nmodels/embedding-gecko-001\nmodels/gemini-1.0-pro-latest\nmodels/gemini-1.0-pro\nmodels/gemini-pro\nmodels/gemini-1.0-pro-001\nmodels/gemini-1.0-pro-vision-latest\nmodels/gemini-pro-vision\nmodels/gemini-1.5-pro-latest\nmodels/gemini-1.5-pro-001\nmodels/gemini-1.5-pro-002\nmodels/gemini-1.5-pro\nmodels/gemini-1.5-pro-exp-0801\nmodels/gemini-1.5-pro-exp-0827\nmodels/gemini-1.5-flash-latest\nmodels/gemini-1.5-flash-001\nmodels/gemini-1.5-flash-001-tuning\nmodels/gemini-1.5-flash\nmodels/gemini-1.5-flash-exp-0827\nmodels/gemini-1.5-flash-002\nmodels/gemini-1.5-flash-8b\nmodels/gemini-1.5-flash-8b-001\nmodels/gemini-1.5-flash-8b-latest\nmodels/gemini-1.5-flash-8b-exp-0827\nmodels/gemini-1.5-flash-8b-exp-0924\nmodels/embedding-001\nmodels/text-embedding-004\nmodels/aqa\n","output_type":"stream"}],"execution_count":12},{"cell_type":"code","source":"for model in genai.list_models():\n  if model.name == 'models/gemini-1.5-flash':\n    print(model)\n    break","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-12T18:58:05.169864Z","iopub.execute_input":"2024-11-12T18:58:05.170271Z","iopub.status.idle":"2024-11-12T18:58:05.231856Z","shell.execute_reply.started":"2024-11-12T18:58:05.170230Z","shell.execute_reply":"2024-11-12T18:58:05.230680Z"}},"outputs":[{"name":"stdout","text":"Model(name='models/gemini-1.5-flash',\n      base_model_id='',\n      version='001',\n      display_name='Gemini 1.5 Flash',\n      description='Fast and versatile multimodal model for scaling across diverse tasks',\n      input_token_limit=1000000,\n      output_token_limit=8192,\n      supported_generation_methods=['generateContent', 'countTokens'],\n      temperature=1.0,\n      max_temperature=2.0,\n      top_p=0.95,\n      top_k=40)\n","output_type":"stream"}],"execution_count":13},{"cell_type":"code","source":"short_model = genai.GenerativeModel(\n    'gemini-1.5-flash',\n    generation_config=genai.GenerationConfig(max_output_tokens=200))\n\nresponse = short_model.generate_content('Write a 1000 word essay on the importance of olives in modern society.')\nprint(response.text)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-12T18:58:20.842967Z","iopub.execute_input":"2024-11-12T18:58:20.843367Z","iopub.status.idle":"2024-11-12T18:58:22.087513Z","shell.execute_reply.started":"2024-11-12T18:58:20.843328Z","shell.execute_reply":"2024-11-12T18:58:22.086451Z"}},"outputs":[{"name":"stdout","text":"## The Enduring Allure of the Olive: A Pillar of Modern Society\n\nFrom the sun-drenched groves of the Mediterranean to the bustling markets of the world, the olive has stood as a testament to the enduring power of nature and human ingenuity. Far more than just a fruit, the olive is a cultural icon, an economic powerhouse, and a culinary staple that has profoundly shaped the fabric of modern society. Its significance extends beyond its delicious flavor and nutritional value, encompassing aspects of health, culture, tradition, and sustainability.\n\nThe olive's journey into the modern world began millennia ago, with its origins traced back to the Middle East. Ancient civilizations recognized its versatility, utilizing its fruit, leaves, and oil for culinary, medicinal, and even religious purposes. This multi-faceted nature has ensured its lasting presence, making it a resilient and indispensable element of human life.\n\nFirstly, the olive's contribution to **health and well-being** cannot be overstated. Its rich content\n","output_type":"stream"}],"execution_count":14},{"cell_type":"code","source":"response = short_model.generate_content('Write a short poem on the importance of olives in modern society.')\nprint(response.text)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-12T18:58:36.451558Z","iopub.execute_input":"2024-11-12T18:58:36.452541Z","iopub.status.idle":"2024-11-12T18:58:37.283374Z","shell.execute_reply.started":"2024-11-12T18:58:36.452494Z","shell.execute_reply":"2024-11-12T18:58:37.282338Z"}},"outputs":[{"name":"stdout","text":"A tiny fruit, a briny taste,\nFrom olive groves, a bounty laced.\nIn salads green, on pizzas bright,\nA vibrant oil, a shining light.\n\nFrom ancient times, a symbol strong,\nOf peace and life, where they belong.\nNow on our plates, a daily treat,\nThe olive's charm, we can't defeat.\n\nA Mediterranean treasure, true,\nIn every dish, its flavors accrue.\nSo raise a glass, to this small tree,\nThe olive's grace, for you and me. \n\n","output_type":"stream"}],"execution_count":15},{"cell_type":"code","source":"from google.api_core import retry\n\nhigh_temp_model = genai.GenerativeModel(\n    'gemini-1.5-flash',\n    generation_config=genai.GenerationConfig(temperature=2.0))\n\n\n# When running lots of queries, it's a good practice to use a retry policy so your code\n# automatically retries when hitting Resource Exhausted (quota limit) errors.\nretry_policy = {\n    \"retry\": retry.Retry(predicate=retry.if_transient_error, initial=10, multiplier=1.5, timeout=300)\n}\n\nfor _ in range(5):\n  response = high_temp_model.generate_content('Pick a random colour... (respond in a single word)',\n                                              request_options=retry_policy)\n  if response.parts:\n    print(response.text, '-' * 25)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-12T19:11:06.557595Z","iopub.execute_input":"2024-11-12T19:11:06.558304Z","iopub.status.idle":"2024-11-12T19:11:15.343445Z","shell.execute_reply.started":"2024-11-12T19:11:06.558264Z","shell.execute_reply":"2024-11-12T19:11:15.342107Z"}},"outputs":[{"name":"stdout","text":"Teal \n -------------------------\nBlue \n -------------------------\nBlue. \n -------------------------\nBlue. \n -------------------------\nBlue \n -------------------------\n","output_type":"stream"}],"execution_count":16},{"cell_type":"code","source":"low_temp_model = genai.GenerativeModel(\n    'gemini-1.5-flash',\n    generation_config=genai.GenerationConfig(temperature=0.0))\n\nfor _ in range(5):\n  response = low_temp_model.generate_content('Pick a random colour... (respond in a single word)',\n                                             request_options=retry_policy)\n  if response.parts:\n    print(response.text, '-' * 25)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-12T19:11:22.679145Z","iopub.execute_input":"2024-11-12T19:11:22.679972Z","iopub.status.idle":"2024-11-12T19:11:23.955968Z","shell.execute_reply.started":"2024-11-12T19:11:22.679928Z","shell.execute_reply":"2024-11-12T19:11:23.954494Z"}},"outputs":[{"name":"stdout","text":"Purple \n -------------------------\nPurple \n -------------------------\nPurple \n -------------------------\nPurple \n -------------------------\nPurple \n -------------------------\n","output_type":"stream"}],"execution_count":17},{"cell_type":"code","source":"model = genai.GenerativeModel(\n    'gemini-1.5-flash-001',\n    generation_config=genai.GenerationConfig(\n        # These are the default values for gemini-1.5-flash-001.\n        temperature=1.0,\n        top_k=64,\n        top_p=0.95,\n    ))\n\nstory_prompt = \"You are a creative writer. Write a short story about a cat who goes on an adventure.\"\nresponse = model.generate_content(story_prompt, request_options=retry_policy)\nprint(response.text)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-12T19:11:39.819893Z","iopub.execute_input":"2024-11-12T19:11:39.820310Z","iopub.status.idle":"2024-11-12T19:11:42.163736Z","shell.execute_reply.started":"2024-11-12T19:11:39.820271Z","shell.execute_reply":"2024-11-12T19:11:42.162587Z"}},"outputs":[{"name":"stdout","text":"Barnaby wasn't your typical house cat. He wasn't content with sunbeams and yarn balls. He yearned for adventure, for the unknown, for something beyond the four walls of his cozy home. One day, a tattered map blew in through the open window, landing at Barnaby's paws. It was a map of the garden, a world filled with mysteries he'd only glimpsed from the windowsill. \n\nBarnaby, his eyes gleaming with anticipation, set off on his quest. He navigated the towering hedges, his whiskers twitching with every new scent. He crossed the stream, his paws cautiously testing the icy water. The garden, he discovered, was a hidden kingdom, teeming with life. A ladybug army marched across a leaf, a family of sparrows argued over a fallen seed, and a plump worm wriggled through the damp earth.\n\nHe encountered obstacles: a grumpy old squirrel who challenged him to a staring contest, a mischievous robin who stole his prized feather toy, and a patch of prickly rose bushes that he navigated with a combination of cunning and agility. But Barnaby was undeterred. He was determined to reach the legendary \"Whispering Willow,\" the oldest tree in the garden, said to hold secrets whispered by the wind.\n\nAs he approached the willow, its branches whispered tales of forgotten times, of playful breezes and silent nights. Barnaby felt a sense of peace, of belonging. He had faced his fears, conquered his anxieties, and discovered a world hidden within the familiar.\n\nAs the sun began to set, casting long shadows across the garden, Barnaby returned home, his heart brimming with newfound confidence. He had faced the unknown and emerged a hero, his whiskers now holding a glimmer of wisdom. He knew, now, that the world was a place of wonder, filled with both dangers and delights. He knew, too, that even a small cat could have grand adventures. And as he curled up on his favourite cushion, Barnaby dreamt of the day he would set off on his next great journey, a journey of discovery, a journey of his own making. \n\n","output_type":"stream"}],"execution_count":18},{"cell_type":"code","source":"model = genai.GenerativeModel(\n    'gemini-1.5-flash-001',\n    generation_config=genai.GenerationConfig(\n        temperature=0.1,\n        top_p=1,\n        max_output_tokens=5,\n    ))\n\nzero_shot_prompt = \"\"\"Classify movie reviews as POSITIVE, NEUTRAL or NEGATIVE.\nReview: She liked everything I bought her \"\"\"\n\nresponse = model.generate_content(zero_shot_prompt, request_options=retry_policy)\nprint(response.text)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-12T19:12:58.838719Z","iopub.execute_input":"2024-11-12T19:12:58.839680Z","iopub.status.idle":"2024-11-12T19:12:59.112423Z","shell.execute_reply.started":"2024-11-12T19:12:58.839637Z","shell.execute_reply":"2024-11-12T19:12:59.111310Z"}},"outputs":[{"name":"stdout","text":"POSITIVE \n\n","output_type":"stream"}],"execution_count":20},{"cell_type":"code","source":"import enum\n\nclass Sentiment(enum.Enum):\n    POSITIVE = \"positive\"\n    NEUTRAL = \"neutral\"\n    NEGATIVE = \"negative\"\n\n\nmodel = genai.GenerativeModel(\n    'gemini-1.5-flash-001',\n    generation_config=genai.GenerationConfig(\n        response_mime_type=\"text/x.enum\",\n        response_schema=Sentiment\n    ))\n\nresponse = model.generate_content(zero_shot_prompt, request_options=retry_policy)\nprint(response.text)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-12T19:13:21.068909Z","iopub.execute_input":"2024-11-12T19:13:21.070189Z","iopub.status.idle":"2024-11-12T19:13:21.542732Z","shell.execute_reply.started":"2024-11-12T19:13:21.070143Z","shell.execute_reply":"2024-11-12T19:13:21.541692Z"}},"outputs":[{"name":"stdout","text":"positive\n","output_type":"stream"}],"execution_count":21},{"cell_type":"code","source":"model = genai.GenerativeModel(\n    'gemini-1.5-flash-latest',\n    generation_config=genai.GenerationConfig(\n        temperature=0.1,\n        top_p=1,\n        max_output_tokens=250,\n    ))\n\nfew_shot_prompt = \"\"\"Parse a customer's pizza order into valid JSON:\n\nEXAMPLE:\nI want a small pizza with cheese, tomato sauce, and pepperoni.\nJSON Response:\n```\n{\n\"size\": \"small\",\n\"type\": \"normal\",\n\"ingredients\": [\"cheese\", \"tomato sauce\", \"peperoni\"]\n}\n```\n\nEXAMPLE:\nCan I get a large pizza with tomato sauce, basil and mozzarella\nJSON Response:\n```\n{\n\"size\": \"large\",\n\"type\": \"normal\",\n\"ingredients\": [\"tomato sauce\", \"basil\", \"mozzarella\"]\n}\n\nORDER:\n\"\"\"\n\ncustomer_order = \"Give me a large with cheese & pineapple\"\n\n\nresponse = model.generate_content([few_shot_prompt, customer_order], request_options=retry_policy)\nprint(response.text)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-12T19:13:46.117884Z","iopub.execute_input":"2024-11-12T19:13:46.118582Z","iopub.status.idle":"2024-11-12T19:13:46.492491Z","shell.execute_reply.started":"2024-11-12T19:13:46.118540Z","shell.execute_reply":"2024-11-12T19:13:46.491421Z"}},"outputs":[{"name":"stdout","text":"```json\n{\n\"size\": \"large\",\n\"type\": \"normal\",\n\"ingredients\": [\"cheese\", \"pineapple\"]\n}\n``` \n\n","output_type":"stream"}],"execution_count":22},{"cell_type":"code","source":"import typing_extensions as typing\n\nclass PizzaOrder(typing.TypedDict):\n    size: str\n    ingredients: list[str]\n    type: str\n\n\nmodel = genai.GenerativeModel(\n    'gemini-1.5-flash-latest',\n    generation_config=genai.GenerationConfig(\n        temperature=0.1,\n        response_mime_type=\"application/json\",\n        response_schema=PizzaOrder,\n    ))\n\nresponse = model.generate_content(\"Can I have a large dessert pizza with apple and chocolate\")\nprint(response.text)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-12T19:14:16.487898Z","iopub.execute_input":"2024-11-12T19:14:16.488762Z","iopub.status.idle":"2024-11-12T19:14:16.882276Z","shell.execute_reply.started":"2024-11-12T19:14:16.488716Z","shell.execute_reply":"2024-11-12T19:14:16.881149Z"}},"outputs":[{"name":"stdout","text":"{\"ingredients\": [\"apple\", \"chocolate\"], \"size\": \"large\", \"type\": \"dessert\"}\n\n","output_type":"stream"}],"execution_count":23},{"cell_type":"code","source":"prompt = \"\"\"When I was 4 years old, my partner was 3 times my age. Now, I\nam 20 years old. How old is my partner? Return the answer immediately.\"\"\"\n\nmodel = genai.GenerativeModel('gemini-1.5-flash-latest')\nresponse = model.generate_content(prompt, request_options=retry_policy)\n\nprint(response.text)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-12T19:14:41.253029Z","iopub.execute_input":"2024-11-12T19:14:41.253465Z","iopub.status.idle":"2024-11-12T19:14:41.599412Z","shell.execute_reply.started":"2024-11-12T19:14:41.253424Z","shell.execute_reply":"2024-11-12T19:14:41.598436Z"}},"outputs":[{"name":"stdout","text":"52 \n\n","output_type":"stream"}],"execution_count":24},{"cell_type":"code","source":"prompt = \"\"\"When I was 4 years old, my partner was 3 times my age. Now,\nI am 20 years old. How old is my partner? Let's think step by step.\"\"\"\n\nresponse = model.generate_content(prompt, request_options=retry_policy)\nprint(response.text)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-12T19:17:33.598937Z","iopub.execute_input":"2024-11-12T19:17:33.599945Z","iopub.status.idle":"2024-11-12T19:17:34.306311Z","shell.execute_reply.started":"2024-11-12T19:17:33.599898Z","shell.execute_reply":"2024-11-12T19:17:34.305162Z"}},"outputs":[{"name":"stdout","text":"Here's how to solve this:\n\n* **When you were 4, your partner was 3 times your age:**  4 years old * 3 = 12 years old\n* **Age difference:** Your partner is 12 - 4 = 8 years older than you.\n* **Your partner's current age:** Since you are now 20, your partner is 20 + 8 = **28 years old.** \n\n","output_type":"stream"}],"execution_count":30},{"cell_type":"code","source":"model_instructions = \"\"\"\nSolve a question answering task with interleaving Thought, Action, Observation steps. Thought can reason about the current situation,\nObservation is understanding relevant information from an Action's output and Action can be one of three types:\n (1) <search>entity</search>, which searches the exact entity on Wikipedia and returns the first paragraph if it exists. If not, it\n     will return some similar entities to search and you can try to search the information from those topics.\n (2) <lookup>keyword</lookup>, which returns the next sentence containing keyword in the current context. This only does exact matches,\n     so keep your searches short.\n (3) <finish>answer</finish>, which returns the answer and finishes the task.\n\"\"\"\n\nexample1 = \"\"\"Question\nMusician and satirist Allie Goertz wrote a song about the \"The Simpsons\" character Milhouse, who Matt Groening named after who?\n\nThought 1\nThe question simplifies to \"The Simpsons\" character Milhouse is named after who. I only need to search Milhouse and find who it is named after.\n\nAction 1\n<search>Milhouse</search>\n\nObservation 1\nMilhouse Mussolini Van Houten is a recurring character in the Fox animated television series The Simpsons voiced by Pamela Hayden and created by Matt Groening.\n\nThought 2\nThe paragraph does not tell who Milhouse is named after, maybe I can look up \"named after\".\n\nAction 2\n<lookup>named after</lookup>\n\nObservation 2\nMilhouse was named after U.S. president Richard Nixon, whose middle name was Milhous.\n\nThought 3\nMilhouse was named after U.S. president Richard Nixon, so the answer is Richard Nixon.\n\nAction 3\n<finish>Richard Nixon</finish>\n\"\"\"\n\nexample2 = \"\"\"Question\nWhat is the elevation range for the area that the eastern sector of the Colorado orogeny extends into?\n\nThought 1\nI need to search Colorado orogeny, find the area that the eastern sector of the Colorado orogeny extends into, then find the elevation range of the area.\n\nAction 1\n<search>Colorado orogeny</search>\n\nObservation 1\nThe Colorado orogeny was an episode of mountain building (an orogeny) in Colorado and surrounding areas.\n\nThought 2\nIt does not mention the eastern sector. So I need to look up eastern sector.\n\nAction 2\n<lookup>eastern sector</lookup>\n\nObservation 2\nThe eastern sector extends into the High Plains and is called the Central Plains orogeny.\n\nThought 3\nThe eastern sector of Colorado orogeny extends into the High Plains. So I need to search High Plains and find its elevation range.\n\nAction 3\n<search>High Plains</search>\n\nObservation 3\nHigh Plains refers to one of two distinct land regions\n\nThought 4\nI need to instead search High Plains (United States).\n\nAction 4\n<search>High Plains (United States)</search>\n\nObservation 4\nThe High Plains are a subregion of the Great Plains. From east to west, the High Plains rise in elevation from around 1,800 to 7,000 ft (550 to 2,130m).\n\nThought 5\nHigh Plains rise in elevation from around 1,800 to 7,000 ft, so the answer is 1,800 to 7,000 ft.\n\nAction 5\n<finish>1,800 to 7,000 ft</finish>\n\"\"\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-12T19:17:45.578206Z","iopub.execute_input":"2024-11-12T19:17:45.579007Z","iopub.status.idle":"2024-11-12T19:17:45.585525Z","shell.execute_reply.started":"2024-11-12T19:17:45.578964Z","shell.execute_reply":"2024-11-12T19:17:45.584448Z"}},"outputs":[],"execution_count":31},{"cell_type":"code","source":"question = \"\"\"Question\nWho was the youngest author listed on the transformers NLP paper?\n\"\"\"\n\nmodel = genai.GenerativeModel('gemini-1.5-flash-latest')\nreact_chat = model.start_chat()\n\n# You will perform the Action, so generate up to, but not including, the Observation.\nconfig = genai.GenerationConfig(stop_sequences=[\"\\nObservation\"])\n\nresp = react_chat.send_message(\n    [model_instructions, example1, example2, question],\n    generation_config=config,\n    request_options=retry_policy)\nprint(resp.text)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-12T19:18:05.240419Z","iopub.execute_input":"2024-11-12T19:18:05.241239Z","iopub.status.idle":"2024-11-12T19:18:06.496766Z","shell.execute_reply.started":"2024-11-12T19:18:05.241195Z","shell.execute_reply":"2024-11-12T19:18:06.495680Z"}},"outputs":[{"name":"stdout","text":"Thought 1\nI need to find the transformers NLP paper and then find the youngest author listed.\n\nAction 1\n<search>transformers NLP paper</search>\n\n","output_type":"stream"}],"execution_count":32},{"cell_type":"code","source":"observation = \"\"\"Observation 1\n[1706.03762] Attention Is All You Need\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, Illia Polosukhin\nWe propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely.\n\"\"\"\nresp = react_chat.send_message(observation, generation_config=config, request_options=retry_policy)\nprint(resp.text)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-12T19:18:20.883521Z","iopub.execute_input":"2024-11-12T19:18:20.883945Z","iopub.status.idle":"2024-11-12T19:18:21.388215Z","shell.execute_reply.started":"2024-11-12T19:18:20.883903Z","shell.execute_reply":"2024-11-12T19:18:21.387117Z"}},"outputs":[{"name":"stdout","text":"Thought 2\nThe observation only lists the authors. I need to find their ages to determine the youngest one.\n\nAction 2\n<search>Ashish Vaswani age</search> \n\n","output_type":"stream"}],"execution_count":33},{"cell_type":"code","source":"model = genai.GenerativeModel(\n    'gemini-1.5-flash-latest',\n    generation_config=genai.GenerationConfig(\n        temperature=1,\n        top_p=1,\n        max_output_tokens=1024,\n    ))\n\n# Gemini 1.5 models are very chatty, so it helps to specify they stick to the code.\ncode_prompt = \"\"\"\nWrite a Python function to calculate the factorial of a number. No explanation, provide only the code.\n\"\"\"\n\nresponse = model.generate_content(code_prompt, request_options=retry_policy)\nMarkdown(response.text)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-12T19:18:36.224022Z","iopub.execute_input":"2024-11-12T19:18:36.224438Z","iopub.status.idle":"2024-11-12T19:18:36.592428Z","shell.execute_reply.started":"2024-11-12T19:18:36.224399Z","shell.execute_reply":"2024-11-12T19:18:36.591422Z"}},"outputs":[{"execution_count":34,"output_type":"execute_result","data":{"text/plain":"<IPython.core.display.Markdown object>","text/markdown":"```python\ndef factorial(n):\n  if n == 0:\n    return 1\n  else:\n    return n * factorial(n-1)\n```"},"metadata":{}}],"execution_count":34},{"cell_type":"code","source":"model = genai.GenerativeModel(\n    'gemini-1.5-flash-latest',\n    tools='code_execution',)\n\ncode_exec_prompt = \"\"\"\nCalculate the sum of the first 14 prime numbers. Only consider the odd primes, and make sure you count them all.\n\"\"\"\n\nresponse = model.generate_content(code_exec_prompt, request_options=retry_policy)\nMarkdown(response.text)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-12T19:19:03.247923Z","iopub.execute_input":"2024-11-12T19:19:03.248637Z","iopub.status.idle":"2024-11-12T19:19:04.435407Z","shell.execute_reply.started":"2024-11-12T19:19:03.248585Z","shell.execute_reply":"2024-11-12T19:19:04.434446Z"}},"outputs":[{"execution_count":36,"output_type":"execute_result","data":{"text/plain":"<IPython.core.display.Markdown object>","text/markdown":"I will calculate the sum of the first 14 odd prime numbers. \n\nHere is a list of the first 14 odd prime numbers:\n\n2, 3, 5, 7, 11, 13, 17, 19, 23, 29, 31, 37, 41, 43\n\n\n``` python\nprint(2 + 3 + 5 + 7 + 11 + 13 + 17 + 19 + 23 + 29 + 31 + 37 + 41 + 43)\n\n```\n```\n281\n\n```\nThe sum of the first 14 odd prime numbers is 281. \n"},"metadata":{}}],"execution_count":36},{"cell_type":"code","source":"import sympy\n\nprimes = sympy.primerange(0, 100)\nodd_primes = [p for p in primes if p % 2 != 0]\nsum_of_primes = sum(odd_primes[:14])\nprint(f'{sum_of_primes=}')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-12T19:19:29.287430Z","iopub.execute_input":"2024-11-12T19:19:29.288207Z","iopub.status.idle":"2024-11-12T19:19:29.726545Z","shell.execute_reply.started":"2024-11-12T19:19:29.288164Z","shell.execute_reply":"2024-11-12T19:19:29.725474Z"}},"outputs":[{"name":"stdout","text":"sum_of_primes=326\n","output_type":"stream"}],"execution_count":37},{"cell_type":"code","source":"sum_of_primes=326","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-12T19:19:48.757887Z","iopub.execute_input":"2024-11-12T19:19:48.759009Z","iopub.status.idle":"2024-11-12T19:19:48.763513Z","shell.execute_reply.started":"2024-11-12T19:19:48.758953Z","shell.execute_reply":"2024-11-12T19:19:48.762430Z"}},"outputs":[],"execution_count":38},{"cell_type":"code","source":"for part in response.candidates[0].content.parts:\n  print(part)\n  print(\"-----\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-12T19:19:58.659675Z","iopub.execute_input":"2024-11-12T19:19:58.660425Z","iopub.status.idle":"2024-11-12T19:19:58.666290Z","shell.execute_reply.started":"2024-11-12T19:19:58.660366Z","shell.execute_reply":"2024-11-12T19:19:58.665152Z"}},"outputs":[{"name":"stdout","text":"text: \"I will calculate the sum of the first 14 odd prime numbers. \\n\\nHere is a list of the first 14 odd prime numbers:\\n\\n2, 3, 5, 7, 11, 13, 17, 19, 23, 29, 31, 37, 41, 43\\n\\n\"\n\n-----\nexecutable_code {\n  language: PYTHON\n  code: \"\\nprint(2 + 3 + 5 + 7 + 11 + 13 + 17 + 19 + 23 + 29 + 31 + 37 + 41 + 43)\\n\"\n}\n\n-----\ncode_execution_result {\n  outcome: OUTCOME_OK\n  output: \"281\\n\"\n}\n\n-----\ntext: \"The sum of the first 14 odd prime numbers is 281. \\n\"\n\n-----\n","output_type":"stream"}],"execution_count":39},{"cell_type":"code","source":"file_contents = !curl https://raw.githubusercontent.com/magicmonty/bash-git-prompt/refs/heads/master/gitprompt.sh\n\nexplain_prompt = f\"\"\"\nPlease explain what this file does at a very high level. What is it, and why would I use it?\n\n```\n{file_contents}\n```\n\"\"\"\n\nmodel = genai.GenerativeModel('gemini-1.5-flash-latest')\n\nresponse = model.generate_content(explain_prompt, request_options=retry_policy)\nMarkdown(response.text)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-12T19:20:15.821504Z","iopub.execute_input":"2024-11-12T19:20:15.821911Z","iopub.status.idle":"2024-11-12T19:20:18.797379Z","shell.execute_reply.started":"2024-11-12T19:20:15.821871Z","shell.execute_reply":"2024-11-12T19:20:18.796313Z"}},"outputs":[{"execution_count":40,"output_type":"execute_result","data":{"text/plain":"<IPython.core.display.Markdown object>","text/markdown":"This file is a Bash/Zsh script called `bash-git-prompt` that adds Git-related information to your command prompt. \n\nHere's a breakdown of what it does at a high level:\n\n* **Provides Git Status Information:** The script checks if you're in a Git repository. If so, it retrieves information like:\n    * **Branch:** Displays the current Git branch.\n    * **Upstream:** Shows status relative to remote branch (e.g., \"ahead by 2\", \"behind by 1\").\n    * **Changes:** Indicates if there are staged, unstaged, untracked files, or conflicts.\n* **Customizable Prompt:** The script allows you to customize the look and feel of your Git prompt through:\n    * **Themes:** You can choose from pre-defined themes or create your own custom theme.\n    * **Symbols:** You can change the symbols used to represent different Git states (e.g., \"ahead\" can be represented with \"â†‘\").\n    * **Colors:** Set the colors for various Git states (e.g., untracked files in red).\n* **Virtual Environment Integration:** The script integrates with virtual environments like virtualenv, Node.js, and conda, displaying their names in the prompt.\n* **Automatic Updates:** The script automatically fetches remote branches and updates the prompt every few minutes (configurable).\n* **Installation:** You simply source the script in your shell configuration file (e.g., `.bashrc`, `.zshrc`).\n\n**Why Use It?**\n\n* **Enhanced Git Awareness:**  It provides a quick visual overview of your Git status, making it easier to track changes and branches.\n* **Improved Workflow:** The script can help you be more efficient by making it easier to identify which branch you're on and if there are any pending changes.\n* **Visual Cues:** The colors and symbols make it easy to identify different Git states and potential issues, like conflicts.\n* **Customizability:** The script offers a range of customization options so you can personalize the Git prompt to your liking.\n\nEssentially, this script turns your regular command prompt into a powerful tool for managing Git projects. \n"},"metadata":{}}],"execution_count":40}]}